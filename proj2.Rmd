---
title: "STA521 Project2: Cloud Image "
author: "Tianyu Wu(...),  Shiqi Liu(2752089)"
output:
  bookdown::pdf_book: 
    toc: false
    number_sections: false
---

```{r setup, message=F, warning=F, echo=F}
require(tidyverse)
require(rstanarm)
require(magrittr)
require(rstan)
require(plyr)
require(patchwork)
require(scales)
require(psych)
require(GGally)
require(gridExtra)
require(MASS)
require(e1071)
require(adabag)
require(class)
require(caTools)
require(ROCR)
require(caret)
require(rpart)
require(car)
require(FactoMineR)
require(modeest)
require(randomForest)
ggplot2::theme_set(ggplot2::theme_bw())
knitr::opts_chunk$set(fig.align = 'center')
```


## 1 Data Collection and Exploration

### (a)

The key purpose of the paper *Daytime Arctic Cloud Detection Based on Multi-Angle Satellite Data With Case Studies* is to build an algorithm in order to ascertain whether a pixel of satellite images of Arctic is covered by cloud or not. Since the surfaces of cloud in Arctic are similar to those of the Arctic surface, it is challenging to tell the difference between cloud surface and Arctic land surface. The satellite images or the data used by the authors are shot by NASA Terra Satellite. The satellite comprises nine cameras with nine different angles in four spectral bands.

The data are collected from 10 MISR (Multiangle Imaging Spectro Radiometer) orbits of path 26 over the Arctic, northern Greenland, and Baffin Bay. This path is selected due to its richness of surface features. Six data units from each orbit are included in this study, and three of the total 60 units are excluded since the sea ice melts in the summer and affect the MISR operational algorithm. To evaluate the performance of the study, one of the authors hand-labels the image pixels as either clear or cloudy. Around 71.5% (5086002) of the pixels are labeled in total; the others are left unlabeled due to ambiguity.

The authors develop a classifying algorithm using enhances linear correlation matching (ELCM) and quadratic discriminant analysis (QDA) with three features (the linear correlation of radiation measurements from different MISR view directions, the standard deviation of MISR red radiation measurements, and the normalized difference angular index). As a result, the algorithm performs much better than other existing MISR operational algorithms. The study itself is also significant since the whole study only included three features and relatively simple classifying methods to separate clear and cloudy regions. Potentially, a more efficient and accurate classifying algorithm for Arctic cloud will eventually enable the scientiÔ¨Åc community to have more accurate global climate model simulations.

### (b)

Percentages of pixels for different classes of the three images are shown in Figure \@ref(fig:percent-table). In Image1, the percentages of the three classes are relatively even. In Image2, the distribution on the three classes is imbalanced. 43.78% of pixels in Image2 are not cloud, while only 17.77% pixels are cloud. In image3, though it has a more imbalanced distribution compared with Image3, most of the imbalance comes from unlabeled class, which is irrelevant to the training process.

```{r percent-table,fig.cap="Percentage of Percentage of not cloud, unlabeled and cloud for each image.",echo=FALSE,out.width="50%"}
knitr::include_graphics("plot/percent-table.png") 
```

Well-labeled maps are shown in Figure \@ref(fig:labeled-images). Based on the labeled map, the classification of data points shows a sticky pattern in all three images. Meanwhile, cloudy regions and clear regions are separated by unlabeled data points. Therefore, i.i.d. assumption can't be justified for this data set.

```{r labeled-images,fig.cap="Image1-3 with expert labelled.",echo=FALSE}
knitr::include_graphics("plot/labeled-images.png") 
```

### (c)

To perform a visual and quantitative EDA on the data set, we first combine the three data sets and remove all unlabeled data points. Then, we generate a plot (in Figure \@ref(fig:pairwise-correlations)) with pairwise relationships between all eight features and a group of box plots (in Figure \@ref(fig:label-feature1)) to show the relationship between each feature and the expert labels. 

In the upper part of the Figure \@ref(fig:pairwise-correlations), three different correlations are presented for each pair of the eight features. The first of the three correlation is computed with the whole data set. The second one is computed with data points labelled as not cloud and the third one is computed with data points labelled as cloud. We observe that the correlations between pairs of the five radiation values are relatively higher than the correlations between pairs of NDAI, SD and CORR. We also find that the correlations between pairs of NDAI, SD and CORR are smaller, when the data set are divided into two groups. The reason for this phenomenon may be that the ranges for the two groups on NDAI, SD and CORR are different.

```{r pairwise-correlations,fig.cap="Pairwise relationships between the three features", echo=FALSE,out.width="100%"}
knitr::include_graphics("plot/pairwise-correlations.png") 
```

From the first plot in Figure \@ref(fig:label-feature1) and the first diagonal plot in Figure\@ref(fig:pairwise-correlations), we find that the distribution of NDAI for the two groups are quite different with each others, so NDAI is supposed to be a good feature to separate data set. For CORR and SD, the differences are relatively smaller. For the five radiation values, DF, CF,BF,AF, and AN, their distributions are similar with each others for both classes. Also, with unlabeled data points removed, the correlations between Label and each of the eight features increase by varying degrees.

```{r label-feature1,fig.cap="Boxplots of label verse each feature, with unlabelled data removed. Correlations of label and each feature with and without unlabeled data. ", echo=FALSE}
knitr::include_graphics("plot/label-feature1.png") 
```

## 2 Preparation

### (a)

For the three images in Figure \@ref(fig:labeled-images), each of them may have a unique pattern. In case of over-fitting to one particular image, the combination of the three images is necessary before training. We also notice that there are some strong dependencies between one pixel and its neighbors. Therefore, we first choose to split the data set by block splitting, to avoid breaking its spatial structure. To be specific, we first divide the data set into numerous $8\times 8$ pixel blocks and for pixels in the same $8\times 8$ block, they are all assigned to one single fold randomly chosen from $K$ folds, and the default value of $K$ in our splitting function is $6$, and unlabeled points are removed. Also, we do not desire that blocks at same location of different images are in the same fold, so the three data sets are splitted separately before combination. Splitted images are shown in Figure \@ref(fig:split-result1). However, as a result of block splitting, ratios of the two labels are not even among different folds. 

```{r split-result1,fig.cap="Splitting result of block splitting.",out.width="100%",echo=FALSE}
knitr::include_graphics("plot/split-result1.png") 
```

For the second splitting method, we expect that each fold has the same ratio of cloud and not cloud data points as the whole data set, which is $61.08%$ for not cloud and $38.92%$ for cloud. To achieve that goal, the second method first divides the data set into cloud and not cloud data sets. Then, each point in the two data sets is evenly and randomly assigned to one out of $K$ folds, and the default value of $K$ is $6$. The percentages of cloud and not cloud for all six folds are shown in Figure \@ref(fig:split-persentage). All folds have $61.08%$ for not cloud and $38.92%$ for cloud.

After all, for both splitting methods, all data points in fold $K$ are used for testing data, all data points in fold $K-1$ are used for validation, and the rest are used for training.


```{r split-persentage,fig.cap="Percentages of cloud and not cloud for each fold generated by the second split method.",echo=FALSE,out.width="40%"}
knitr::include_graphics("plot/split-persentage.png") 
```

### (b)

For test and validation data sets using either one of the two splitting methods, the percentage of not cloud is all around $60%$ (in Figure \@ref(fig:trivial-classifier)). Therefore, the accuracy of a trivial classifier, which sets all labels to $-1$, would also be around $60%$ for validation sets and test sets, regardless of splitting methods. The accuracy of a trivial classifier would be high, if the data set is highly imbalanced. For example, if $99%$ of the data set are labelled as not cloud, the accuracy of the trivial classifier would be $99%$. Concretely, the trivial classifier is not well-performed for our data set, so more fancier classifiers are necessary to achieve a higher accuracy. 

```{r trivial-classifier,fig.cap="Percentage of not cloud and cloud for validation set and test set of two split method.",out.width="60%",echo=FALSE}
knitr::include_graphics("plot/trivial-classifier.png") 
```

### (c)

By performing the logistic regression forward selection on all features (in Figure \@ref(fig:best-features)), NDAI, CORR, and SD are three of the "best" features. With adding NDAI, CORR, and SD to the training model, AIC and deviance drops significantly. According to *Daytime Arctic Cloud Detection Based on Multi-Angle Satellite Data With Case Studies*, these three features are computed with all radiations of nine angels, so these three features include some information of the other features, DF, BF, AN, CF and AF. Also, these three features of each pixel are based on the radiation values of pixels around itself, so these three features also include some information of its neighbors. Choosing these three features, our model would be more likely to classify closep data points into the same category. In the following questions, only these three features are used to train classification models.

```{r best-features,fig.cap="Result of logistic regression forward selection on all features." ,out.width="60%",echo=FALSE}
knitr::include_graphics("plot/best-features.png") 
```

### (d)

CVmaster function is in CVmaster.R.

## 3 Modeling

### (a)

Selected classification models are Logistic Regression(LR), Linear Discriminant Analysis(LDA), Quadratic Discriminant Analysis(QDA), Naive Bayes(NB) and Classification and Regression Tree(CART). Logistic Regression is a classification model with mainly two assumptions. The first is that the response variable (class) follows a Bernoulli distribution. The second is that the log-odds of this Bernoulli variable is linear with respect to covariates. This linear assumption, as indicated by part4(a) in the next section, is reasonable but might not be accurate. Assumptions for LDA and QDA are the same: there is a data generating process behind the scenes. First, generate a class. Second, generate instances for the chosen class. Within each class, instances are normal distributed with different means. The difference of LDA and QDA lies in the fact that the covariance for different classes are the same for LDA while the covariance are different across classes for QDA. There is also a data generation process behind NB. The assumption here is that coviarates are generated independently within each class. As can be seen from Figure \@ref(fig:density), distributions of covariates are not normal within each class, nor are they independent in general. Some of the distributions are bimodal, which are far from Gaussian distribution while the distributions of NDAI and SD are quite close to Gaussian. The dependence of covariates can also be seen from GGPair plot as in Figure \@ref(fig:pairwise-correlations). It is interesting to observe that within each class the correlation between covaraites are actually smaller. The independence assumption of Naive Bayes is not perfectly accurate, but reasonable. For accuracy results, we can see that cross-validation mean accuracy is very close to test accuracy in general. This indicates that CV is a robust way of evaluating model performance. The top model is Naive Bayes according to test accuracy.

```{r cv-acc,fig.cap="The accuracies across folds, the average accuracies across folds, and the test accurary for each classification model with two different splitting methods." ,out.width="100%",echo=FALSE}
knitr::include_graphics("plot/cv-acc.png") 
```

```{r density,fig.cap="Distribution of covariates",out.width="70%",echo=FALSE}
knitr::include_graphics("plot/density.png") 
```

### (b)

Cutoff values are selected based on maximum Youden's J Statistic (Youden's Index), which is defined as True Positive Rate(TPR) - False Positive Rate(FPR). Geometrically it indicates the vertical distance right above the 45 degree line which represents ROC curve for a basic random model. As can be seen from the Figure \@ref(fig:roc), the selected cut-off points locate almost at the top-left corner of each ROC curve. This is preferred as we want to maintain relatively high TPR and low FPR at the same time.

```{r roc,fig.cap="ROC curve with cut-off values",out.width="70%",echo=FALSE}
knitr::include_graphics("plot/roc.png") 
```

### (c)

Other selected relevant metrics are precision, recall, f1 score and area under curve(AUC). Precision measures the ratio of correct prediction out of all positive predictions. In other words, precision measures quality of positive prediction. Recall measures the ratio of successfully predicted positive instances out of all positive instances. In other words, recall measures the ability to identify positive instances. F1 score is the harmonic mean of precision and recall, which can be seen as a comprehensive measurement regarding positive predictions combining both precision and recall. AUC measures area under ROC curve, which can be seen as a comprehensive measurement regarding the accuracy of positive predictions. Best model is again Naive Bayes, according to f1 score and AUC (Table \@ref(fig:prf)).

```{r prf,fig.cap="Distribution of covariates",out.width="70%",echo=FALSE}
knitr::include_graphics("plot/test-prf.png") 
```

## 4 Diagnostics

### (a)

From optimization perspective, the objective function of logistic regression loss is convex. According to Table \@ref(fig:lr-smr), parameters converged after 6 steps and achieves global minimum. From the effectiveness perspective, the model works effectively as there is a huge drop from null deviance to residual deviance, indicating tremendous gain from logistic regression model compared to a plain constant model. From validity perspective, all the coefficients are statistically significant. The p-values are too small that they become numerically 0 (Table \@ref(fig:lr-coef). To test the linear assumption, scatter plots of predicted log-odds and covariates (Figure \@ref(fig:lr-linearity)) are plotted. Strict linearity should not be expected as the linearity is with respect to all covariates while each scatter plots only demonstrate the relationship between one log-odds and one variable. Therefore, a smooth estimation is made to demonstrate the trend. As can be seen, there is linear relationship with respect to NDAI and CORR, but not for SD. Training data does not align with linear assumption perfectly. Besides linearity assumption, collinearity is also crucial for linear models. We adopted variance inflation factors to test collinearity among covariates (Table \@ref(fig:lr-collinearity)). Since as a rule of thumb VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity, there is not collinearity issue in training data. In the end, we try to visualize decision boundary in two-dimension space using PCA, but the projected data indicates that there does not exists such boundary in lower dimensional space (Figure \@ref(fig:lr-pca)). The evidence is that predicted positive and negative labels mix together. We cannot determine the predicted label of a given lower-space point. This makes sense if we notice that the decision boundary in 3-dimensional space is 2-dimensional, the projection of which should also be 2-dimensional.

```{r lr-smr,fig.cap="Summary of logistic regression model",out.width="70%",echo=FALSE}
knitr::include_graphics("plot/lr_smr.png") 
```

```{r lr-coef,fig.cap="Coefficients of logistic regression model",out.width="70%",echo=FALSE}
knitr::include_graphics("plot/lr_coef.png") 
```

```{r lr-linearity,fig.cap="Linearity test for logistic regression model",out.width="70%",echo=FALSE}
knitr::include_graphics("plot/lr_linearity.png") 
```

```{r lr-collinearity,fig.cap="Collinearity test for logistic regression model",out.width="70%",echo=FALSE}
knitr::include_graphics("plot/lr_collinearity.png") 
```

```{r lr-pca,fig.cap="PCA for logistic regression model",out.width="70%",echo=FALSE}
knitr::include_graphics("plot/lr_pca.png") 
```

### (b)











